{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Reduction Techniques\n",
    "\n",
    "This notebook will showcase several common techniques to reduce metric variance, which is used to increase metric sensitivity for AB testing. The dataset to be investigated with is provided by Starbucks and shared within the Data Scientist Nano-degree program. It contains customer promotion and purchase data, along with seven measures. You can know more about it by visiting this [link](https://drive.google.com/file/d/18klca9Sef1Rs6q8DW4l7o349r8B70qXM/view). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# Here is the introduction of this dataset: \n",
    "# https://drive.google.com/file/d/18klca9Sef1Rs6q8DW4l7o349r8B70qXM/view\n",
    "data_set = pd.read_csv('./training_ab_starbucks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.443518</td>\n",
       "      <td>-1.165083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.159350</td>\n",
       "      <td>-0.645617</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.431659</td>\n",
       "      <td>0.133583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.588914</td>\n",
       "      <td>-0.212728</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.044332</td>\n",
       "      <td>-0.385883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Promotion  purchase  V1         V2        V3  V4  V5  V6  V7\n",
       "0   1        No         0   2  30.443518 -1.165083   1   1   3   2\n",
       "1   3        No         0   3  32.159350 -0.645617   2   3   2   2\n",
       "2   4        No         0   2  30.431659  0.133583   1   1   4   2\n",
       "3   5        No         0   0  26.588914 -0.212728   2   1   4   2\n",
       "4   8       Yes         0   3  28.044332 -0.385883   1   1   2   2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0\n",
       "Promotion    0\n",
       "purchase     0\n",
       "V1           0\n",
       "V2           0\n",
       "V3           0\n",
       "V4           0\n",
       "V5           0\n",
       "V6           0\n",
       "V7           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no null value in the dataset\n",
    "data_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of total users\n",
    "nr_users = data_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset contains 84534 customers, in which 42364 of them received promotion and the rest 42170 did not.\n"
     ]
    }
   ],
   "source": [
    "# number of customers who received the promotion or not\n",
    "group_aggr = data_set.groupby(['Promotion']).count().reset_index()\n",
    "group_promoted = group_aggr.loc[group_aggr['Promotion'] == 'Yes']['ID'].iloc[0] # received\n",
    "group_not_promoted = group_aggr.loc[group_aggr['Promotion'] == 'No']['ID'].iloc[0] # not received\n",
    "\n",
    "print(\"This dataset contains {} customers, in which {} of them received promotion and the rest {} did not.\".format(str(nr_users), str(group_promoted), str(group_not_promoted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than that, this dataset also contains seven measures, V1 to V7, and one business metric which tells whether the customer purchase or not. The purpose of this notebook is adopting different variance reduction techniques and look at how much variance each method is able to reduce compared against adopting nothing instead.\n",
    "\n",
    "Bytepawn published a very helpful [article](https://bytepawn.com/five-ways-to-reduce-variance-in-ab-testing.html), which introduced five techniques:\n",
    "\n",
    "1. Increase sample size\n",
    "2. Move towards an even split\n",
    "3. Reduce variance in the metric definition\n",
    "4. Stratification\n",
    "5. CUPED\n",
    "\n",
    "Whay will I do, differently from the article from Bytepawn, is validating these techniques against the real world dataset, rather than simulating the numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the details, let's figure out the date type of each column -\n",
    "\n",
    "We have **Promotion** as a binary data which we can split the users into two groups - control and treatment;\n",
    "\n",
    "**purchase** is another binary data where we know customers made purchase or not. In business, we usually aggregate it into conversion rate to evaluate the performance.\n",
    "\n",
    "For **V1** and **V4** to **V7**, they are all integers, which we will regard them as category data.\n",
    "\n",
    "Lastly, the **V2** and **V3** variables are floats, and we will look at the mean average to evaluate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             int64\n",
       "Promotion     object\n",
       "purchase       int64\n",
       "V1             int64\n",
       "V2           float64\n",
       "V3           float64\n",
       "V4             int64\n",
       "V5             int64\n",
       "V6             int64\n",
       "V7             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn int columns such as 'purchase','V1','V4','V5','V6','V7' to category type\n",
    "categorical_columns = ['purchase','V1','V4','V5','V6','V7']\n",
    "for column in categorical_columns:\n",
    "    data_set[column] = data_set[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534</td>\n",
       "      <td>84534.0</td>\n",
       "      <td>84534.0</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.0</td>\n",
       "      <td>84534.0</td>\n",
       "      <td>84534.0</td>\n",
       "      <td>84534.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>42364</td>\n",
       "      <td>83494.0</td>\n",
       "      <td>31631.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57450.0</td>\n",
       "      <td>32743.0</td>\n",
       "      <td>21186.0</td>\n",
       "      <td>59317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>62970.972413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.973600</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>36418.440539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.010626</td>\n",
       "      <td>1.000485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.104007</td>\n",
       "      <td>-1.684550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31467.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.591501</td>\n",
       "      <td>-0.905350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62827.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.979744</td>\n",
       "      <td>-0.039572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>94438.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.344593</td>\n",
       "      <td>0.826206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>126184.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.375913</td>\n",
       "      <td>1.691984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID Promotion  purchase       V1            V2  \\\n",
       "count    84534.000000     84534   84534.0  84534.0  84534.000000   \n",
       "unique            NaN         2       2.0      4.0           NaN   \n",
       "top               NaN       Yes       0.0      1.0           NaN   \n",
       "freq              NaN     42364   83494.0  31631.0           NaN   \n",
       "mean     62970.972413       NaN       NaN      NaN     29.973600   \n",
       "std      36418.440539       NaN       NaN      NaN      5.010626   \n",
       "min          1.000000       NaN       NaN      NaN      7.104007   \n",
       "25%      31467.250000       NaN       NaN      NaN     26.591501   \n",
       "50%      62827.500000       NaN       NaN      NaN     29.979744   \n",
       "75%      94438.750000       NaN       NaN      NaN     33.344593   \n",
       "max     126184.000000       NaN       NaN      NaN     50.375913   \n",
       "\n",
       "                  V3       V4       V5       V6       V7  \n",
       "count   84534.000000  84534.0  84534.0  84534.0  84534.0  \n",
       "unique           NaN      2.0      4.0      4.0      2.0  \n",
       "top              NaN      2.0      3.0      3.0      2.0  \n",
       "freq             NaN  57450.0  32743.0  21186.0  59317.0  \n",
       "mean        0.000190      NaN      NaN      NaN      NaN  \n",
       "std         1.000485      NaN      NaN      NaN      NaN  \n",
       "min        -1.684550      NaN      NaN      NaN      NaN  \n",
       "25%        -0.905350      NaN      NaN      NaN      NaN  \n",
       "50%        -0.039572      NaN      NaN      NaN      NaN  \n",
       "75%         0.826206      NaN      NaN      NaN      NaN  \n",
       "max         1.691984      NaN      NaN      NaN      NaN  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of the dataset\n",
    "data_set.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance of continous data i.e. V2 and V3\n",
    "variance_float_dataset = data_set.groupby(['Promotion']).var().reset_index()[['Promotion','V2','V3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate variance for categorical variables\n",
    "# this stackoverflow link explains how \n",
    "# https://stats.stackexchange.com/questions/421307/variance-maybe-of-categorical-data\n",
    "# a bigger entropy means a more evenly distributed or a smaller variance of the categorical counts of the variable\n",
    "\n",
    "def category_variance(col):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    an array of categorical values as one variable\n",
    "    \n",
    "    output:\n",
    "    calculate the entropy value, which is the variance for categorical variable\n",
    "    reference: https://stats.stackexchange.com/questions/421307/variance-maybe-of-categorical-data\n",
    "    \"\"\"\n",
    "    category_counts = list(col.value_counts())\n",
    "    total_freq = sum(category_counts)\n",
    "    alpha = 1\n",
    "    probs = []\n",
    "\n",
    "    for count in category_counts:\n",
    "        p = (count + alpha) / (total_freq + len(category_counts) * alpha)\n",
    "        probs.append(p)\n",
    "    \n",
    "    log_sum = 0\n",
    "    for p in probs:\n",
    "        log_sum += p*math.log(p)\n",
    "    \n",
    "    entropy = 0 - log_sum    \n",
    "    return entropy\n",
    "\n",
    "categorical_dataset = data_set[['Promotion','V1','V4','V5','V6','V7']]\n",
    "\n",
    "variance_categorical_dataset = categorical_dataset.groupby(\"Promotion\").agg(category_variance).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance of binary variable, such as the purchase column\n",
    "def binomial_variance(col):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    an array of binomial values as one variable\n",
    "    \n",
    "    output:\n",
    "    return the value of the variance of the array\n",
    "    reference: https://stats.stackexchange.com/questions/191444/variance-in-estimating-p-for-a-binomial-distribution\n",
    "    \"\"\"\n",
    "    value_counts = list(data_set['purchase'].value_counts())\n",
    "    sum_freq = value_counts[0] + value_counts[1]\n",
    "    p = value_counts[0] / sum_freq\n",
    "    variance = (p * (1-p)) / sum_freq\n",
    "    return variance\n",
    "\n",
    "binomial_dataset = data_set[['Promotion','purchase']]\n",
    "variance_bino_dataset = binomial_dataset.groupby(\"Promotion\").agg(binomial_variance).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.257585</td>\n",
       "      <td>24.967657</td>\n",
       "      <td>1.005839</td>\n",
       "      <td>0.626672</td>\n",
       "      <td>1.218653</td>\n",
       "      <td>1.386292</td>\n",
       "      <td>0.608992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.257582</td>\n",
       "      <td>25.245032</td>\n",
       "      <td>0.996043</td>\n",
       "      <td>0.627665</td>\n",
       "      <td>1.214831</td>\n",
       "      <td>1.386286</td>\n",
       "      <td>0.609865</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Promotion      purchase        V1         V2        V3        V4        V5  \\\n",
       "0        No  1.437455e-07  1.257585  24.967657  1.005839  0.626672  1.218653   \n",
       "1       Yes  1.437455e-07  1.257582  25.245032  0.996043  0.627665  1.214831   \n",
       "\n",
       "         V6        V7  \n",
       "0  1.386292  0.608992  \n",
       "1  1.386286  0.609865  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# put variance of each variable together into one table\n",
    "merged = pd.merge(variance_float_dataset, \n",
    "         variance_categorical_dataset, \n",
    "         on='Promotion', how='inner')\n",
    "\n",
    "variance_aggr = pd.merge(merged, \n",
    "         variance_bino_dataset, \n",
    "         on='Promotion', how='inner')\n",
    "\n",
    "# sort the columns\n",
    "variance_aggr = variance_aggr[['Promotion','purchase','V1','V2','V3','V4','V5','V6','V7']]\n",
    "\n",
    "variance_aggr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Increase sample size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Move towards an even split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reduce variance in the metric definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CUPED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the reduced variance of each technique with the original variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
