{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Reduction Techniques\n",
    "\n",
    "This notebook will showcase several common techniques to reduce metric variance, which is used to increase metric sensitivity for AB testing. The dataset to be investigated with is provided by Starbucks and shared within the Data Scientist Nano-degree program. It contains customer promotion and purchase data, along with seven measures. You can know more about it by visiting this [link](https://drive.google.com/file/d/18klca9Sef1Rs6q8DW4l7o349r8B70qXM/view). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "# Here is the introduction of this dataset: \n",
    "# https://drive.google.com/file/d/18klca9Sef1Rs6q8DW4l7o349r8B70qXM/view\n",
    "data_set = pd.read_csv('./training_ab_starbucks.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.443518</td>\n",
       "      <td>-1.165083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.159350</td>\n",
       "      <td>-0.645617</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.431659</td>\n",
       "      <td>0.133583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.588914</td>\n",
       "      <td>-0.212728</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.044332</td>\n",
       "      <td>-0.385883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Promotion  purchase  V1         V2        V3  V4  V5  V6  V7\n",
       "0   1        No         0   2  30.443518 -1.165083   1   1   3   2\n",
       "1   3        No         0   3  32.159350 -0.645617   2   3   2   2\n",
       "2   4        No         0   2  30.431659  0.133583   1   1   4   2\n",
       "3   5        No         0   0  26.588914 -0.212728   2   1   4   2\n",
       "4   8       Yes         0   3  28.044332 -0.385883   1   1   2   2"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID           0\n",
       "Promotion    0\n",
       "purchase     0\n",
       "V1           0\n",
       "V2           0\n",
       "V3           0\n",
       "V4           0\n",
       "V5           0\n",
       "V6           0\n",
       "V7           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no null value in the dataset\n",
    "data_set.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of total users\n",
    "nr_users = data_set.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This dataset contains 84534 customers, in which 42364 of them received promotion and the rest 42170 did not.\n"
     ]
    }
   ],
   "source": [
    "# number of customers who received the promotion or not\n",
    "group_aggr = data_set.groupby(['Promotion']).count().reset_index()\n",
    "group_promoted = group_aggr.loc[group_aggr['Promotion'] == 'Yes']['ID'].iloc[0] # received\n",
    "group_not_promoted = group_aggr.loc[group_aggr['Promotion'] == 'No']['ID'].iloc[0] # not received\n",
    "\n",
    "print(\"This dataset contains {} customers, in which {} of them received promotion and the rest {} did not.\".format(str(nr_users), str(group_promoted), str(group_not_promoted)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other than that, this dataset also contains seven measures, V1 to V7, and one business metric which tells whether the customer purchase or not. The purpose of this notebook is adopting different variance reduction techniques and look at how much variance each method is able to reduce compared against adopting nothing instead.\n",
    "\n",
    "Bytepawn published a very helpful [article](https://bytepawn.com/five-ways-to-reduce-variance-in-ab-testing.html), which introduced five techniques:\n",
    "\n",
    "1. Increase sample size\n",
    "2. Move towards an even split\n",
    "3. Reduce variance in the metric definition\n",
    "4. Stratification\n",
    "5. CUPED\n",
    "\n",
    "Whay will I do, differently from the article from Bytepawn, is validating these techniques against the real world dataset, rather than simulating the numbers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before diving into the details, let's figure out the date type of each column -\n",
    "\n",
    "We have **Promotion** as a binary data which we can split the users into two groups - control and treatment;\n",
    "\n",
    "**purchase** is another binary data where we know customers made purchase or not. In business, we usually aggregate it into conversion rate to evaluate the performance.\n",
    "\n",
    "For **V1** and **V4** to **V7**, they are all integers, which we will regard them as category data.\n",
    "\n",
    "Lastly, the **V2** and **V3** variables are floats, and we will look at the mean average to evaluate the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID             int64\n",
       "Promotion     object\n",
       "purchase       int64\n",
       "V1             int64\n",
       "V2           float64\n",
       "V3           float64\n",
       "V4             int64\n",
       "V5             int64\n",
       "V6             int64\n",
       "V7             int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn int columns such as 'purchase','V1','V4','V5','V6','V7' to category type\n",
    "categorical_columns = ['purchase','V1','V4','V5','V6','V7']\n",
    "for column in categorical_columns:\n",
    "    data_set[column] = data_set[column].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify metrincs into different list based on their types\n",
    "mean_metrics = ['V2','V3']\n",
    "binomial_metrics = ['purchase']\n",
    "categorical_metrics = ['V1','V4','V5','V6','V7']\n",
    "aggr_metric = ['Promotion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534</td>\n",
       "      <td>84534.0</td>\n",
       "      <td>84534.0</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.000000</td>\n",
       "      <td>84534.0</td>\n",
       "      <td>84534.0</td>\n",
       "      <td>84534.0</td>\n",
       "      <td>84534.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>NaN</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>NaN</td>\n",
       "      <td>42364</td>\n",
       "      <td>83494.0</td>\n",
       "      <td>31631.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57450.0</td>\n",
       "      <td>32743.0</td>\n",
       "      <td>21186.0</td>\n",
       "      <td>59317.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>62970.972413</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.973600</td>\n",
       "      <td>0.000190</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>36418.440539</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.010626</td>\n",
       "      <td>1.000485</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.104007</td>\n",
       "      <td>-1.684550</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>31467.250000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26.591501</td>\n",
       "      <td>-0.905350</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>62827.500000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>29.979744</td>\n",
       "      <td>-0.039572</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>94438.750000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.344593</td>\n",
       "      <td>0.826206</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>126184.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50.375913</td>\n",
       "      <td>1.691984</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID Promotion  purchase       V1            V2  \\\n",
       "count    84534.000000     84534   84534.0  84534.0  84534.000000   \n",
       "unique            NaN         2       2.0      4.0           NaN   \n",
       "top               NaN       Yes       0.0      1.0           NaN   \n",
       "freq              NaN     42364   83494.0  31631.0           NaN   \n",
       "mean     62970.972413       NaN       NaN      NaN     29.973600   \n",
       "std      36418.440539       NaN       NaN      NaN      5.010626   \n",
       "min          1.000000       NaN       NaN      NaN      7.104007   \n",
       "25%      31467.250000       NaN       NaN      NaN     26.591501   \n",
       "50%      62827.500000       NaN       NaN      NaN     29.979744   \n",
       "75%      94438.750000       NaN       NaN      NaN     33.344593   \n",
       "max     126184.000000       NaN       NaN      NaN     50.375913   \n",
       "\n",
       "                  V3       V4       V5       V6       V7  \n",
       "count   84534.000000  84534.0  84534.0  84534.0  84534.0  \n",
       "unique           NaN      2.0      4.0      4.0      2.0  \n",
       "top              NaN      2.0      3.0      3.0      2.0  \n",
       "freq             NaN  57450.0  32743.0  21186.0  59317.0  \n",
       "mean        0.000190      NaN      NaN      NaN      NaN  \n",
       "std         1.000485      NaN      NaN      NaN      NaN  \n",
       "min        -1.684550      NaN      NaN      NaN      NaN  \n",
       "25%        -0.905350      NaN      NaN      NaN      NaN  \n",
       "50%        -0.039572      NaN      NaN      NaN      NaN  \n",
       "75%         0.826206      NaN      NaN      NaN      NaN  \n",
       "max         1.691984      NaN      NaN      NaN      NaN  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# summary of the dataset\n",
    "data_set.describe(include = 'all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance of continous data i.e. V2 and V3\n",
    "\n",
    "def mean_variance(col):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    df: the dataset we want to calculate variance of mean metrics\n",
    "    aggr: column to aggregate the measures\n",
    "    mean_cols: columns we evaluate the means\n",
    "    \n",
    "    output:\n",
    "    an aggregated dataset showcasing the variance of each mean measures of the df dataset\n",
    "    \"\"\"\n",
    "    variance_float_dataset = col.var()\n",
    "    return variance_float_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate variance for categorical variables\n",
    "# this stackoverflow link explains how \n",
    "# https://stats.stackexchange.com/questions/421307/variance-maybe-of-categorical-data\n",
    "# a bigger entropy means a more evenly distributed or a smaller variance of the categorical counts of the variable\n",
    "\n",
    "def category_variance(col):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    an array of categorical values as one variable\n",
    "    \n",
    "    output:\n",
    "    calculate the entropy value, which is the variance for categorical variable\n",
    "    reference: https://stats.stackexchange.com/questions/421307/variance-maybe-of-categorical-data\n",
    "    \"\"\"\n",
    "    category_counts = list(col.value_counts())\n",
    "    total_freq = sum(category_counts)\n",
    "    alpha = 1\n",
    "    probs = []\n",
    "\n",
    "    for count in category_counts:\n",
    "        p = (count + alpha) / (total_freq + len(category_counts) * alpha)\n",
    "        probs.append(p)\n",
    "    \n",
    "    log_sum = 0\n",
    "    for p in probs:\n",
    "        log_sum += p*math.log(p)\n",
    "    \n",
    "    entropy = 0 - log_sum    \n",
    "    return entropy\n",
    "\n",
    "#categorical_dataset = data_set[['Promotion','V1','V4','V5','V6','V7']]\n",
    "\n",
    "variance_categorical_dataset = categorical_dataset.groupby(\"Promotion\").agg(category_variance).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variance of binary variable, such as the purchase column\n",
    "def binomial_variance(col):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    an array of binomial values as one variable\n",
    "    \n",
    "    output:\n",
    "    return the value of the variance of the array\n",
    "    reference: https://stats.stackexchange.com/questions/191444/variance-in-estimating-p-for-a-binomial-distribution\n",
    "    \"\"\"\n",
    "    value_counts = list(data_set['purchase'].value_counts())\n",
    "    sum_freq = value_counts[0] + value_counts[1]\n",
    "    p = value_counts[0] / sum_freq\n",
    "    variance = (p * (1-p)) / sum_freq\n",
    "    return variance\n",
    "\n",
    "binomial_dataset = data_set[['Promotion','purchase']]\n",
    "variance_bino_dataset = binomial_dataset.groupby(\"Promotion\").agg(binomial_variance).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put variance of each variable together into one table\n",
    "\n",
    "def merged(df1, df2, df3):\n",
    "    \"\"\"\n",
    "    input: dataset we want to merged together\n",
    "    \n",
    "    output: a merged dataset where we have the variance of each measure\n",
    "    \"\"\"\n",
    "    merged_df = pd.merge(df1, \n",
    "         df2, \n",
    "         on='Promotion', how='inner')\n",
    "\n",
    "    df = pd.merge(merged_df, \n",
    "         df3, \n",
    "         on='Promotion', how='inner')\n",
    "    # sort the columns\n",
    "    df = df[['Promotion','purchase','V1','V2','V3','V4','V5','V6','V7']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Increase sample size\n",
    "\n",
    "Regardless it is mean, binomial or categorical data, the variance of each measure is influenced by the sample size.\n",
    "\n",
    "Let's randomly take 25%, 50% and 75% of the dataset and calculate the variance of each metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.443518</td>\n",
       "      <td>-1.165083</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>32.159350</td>\n",
       "      <td>-0.645617</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>30.431659</td>\n",
       "      <td>0.133583</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>No</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.588914</td>\n",
       "      <td>-0.212728</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>28.044332</td>\n",
       "      <td>-0.385883</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID Promotion purchase V1         V2        V3 V4 V5 V6 V7\n",
       "0   1        No        0  2  30.443518 -1.165083  1  1  3  2\n",
       "1   3        No        0  3  32.159350 -0.645617  2  3  2  2\n",
       "2   4        No        0  2  30.431659  0.133583  1  1  4  2\n",
       "3   5        No        0  0  26.588914 -0.212728  2  1  4  2\n",
       "4   8       Yes        0  3  28.044332 -0.385883  1  1  2  2"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sampling(df, prop = 0.25, random_state = 42):\n",
    "    \"\"\"\n",
    "    input: a dataframe and the share we want to randomly sample from the dataset\n",
    "    \n",
    "    output: returned a sampled dataset\n",
    "    \"\"\"\n",
    "    sampled_df = df.sample(frac = prop, random_state = random_state)\n",
    "    \n",
    "    return sampled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of proportions we want to sample the original dataset\n",
    "props = [0.05, 0.25, 0.5, 0.75, 1]\n",
    "\n",
    "# create a dataset containing the variance of each metric at different sampling rate\n",
    "outcome = pd.DataFrame()\n",
    "\n",
    "for prop in props:\n",
    "    df = random_sampling(data_set, prop = prop)\n",
    "    variance_mean_dataset = df[aggr_metric + mean_metrics].groupby(\"Promotion\").agg(mean_variance).reset_index()\n",
    "    variance_categorical_dataset = df[aggr_metric + categorical_metrics].groupby(\"Promotion\").agg(category_variance).reset_index()\n",
    "    variance_binomial_dataset = df[aggr_metric + binomial_metrics].groupby(\"Promotion\").agg(binomial_variance).reset_index()\n",
    "    merged_df = merged(variance_mean_dataset,variance_categorical_dataset,variance_binomial_dataset)\n",
    "    merged_df['prop_sampling'] = prop\n",
    "    merged_df['sample_size'] = df.shape[0]\n",
    "    outcome = outcome.append(merged_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Promotion</th>\n",
       "      <th>purchase</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>prop_sampling</th>\n",
       "      <th>sample_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.256658</td>\n",
       "      <td>26.010979</td>\n",
       "      <td>1.008264</td>\n",
       "      <td>0.619052</td>\n",
       "      <td>1.245876</td>\n",
       "      <td>1.385353</td>\n",
       "      <td>0.605214</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.263755</td>\n",
       "      <td>25.405717</td>\n",
       "      <td>1.012895</td>\n",
       "      <td>0.625228</td>\n",
       "      <td>1.210419</td>\n",
       "      <td>1.386085</td>\n",
       "      <td>0.617773</td>\n",
       "      <td>0.05</td>\n",
       "      <td>4227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.259141</td>\n",
       "      <td>24.944699</td>\n",
       "      <td>1.006074</td>\n",
       "      <td>0.627840</td>\n",
       "      <td>1.222414</td>\n",
       "      <td>1.386248</td>\n",
       "      <td>0.609784</td>\n",
       "      <td>0.25</td>\n",
       "      <td>21134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.255337</td>\n",
       "      <td>25.274806</td>\n",
       "      <td>0.992484</td>\n",
       "      <td>0.628158</td>\n",
       "      <td>1.217535</td>\n",
       "      <td>1.386273</td>\n",
       "      <td>0.609498</td>\n",
       "      <td>0.25</td>\n",
       "      <td>21134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.253815</td>\n",
       "      <td>25.146281</td>\n",
       "      <td>1.000132</td>\n",
       "      <td>0.629407</td>\n",
       "      <td>1.221187</td>\n",
       "      <td>1.386239</td>\n",
       "      <td>0.610107</td>\n",
       "      <td>0.50</td>\n",
       "      <td>42267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.257019</td>\n",
       "      <td>25.110459</td>\n",
       "      <td>0.990591</td>\n",
       "      <td>0.627671</td>\n",
       "      <td>1.216101</td>\n",
       "      <td>1.386261</td>\n",
       "      <td>0.608508</td>\n",
       "      <td>0.50</td>\n",
       "      <td>42267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.254171</td>\n",
       "      <td>25.105386</td>\n",
       "      <td>1.004506</td>\n",
       "      <td>0.626354</td>\n",
       "      <td>1.220354</td>\n",
       "      <td>1.386285</td>\n",
       "      <td>0.609656</td>\n",
       "      <td>0.75</td>\n",
       "      <td>63400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.258327</td>\n",
       "      <td>25.236680</td>\n",
       "      <td>0.994788</td>\n",
       "      <td>0.627159</td>\n",
       "      <td>1.215619</td>\n",
       "      <td>1.386270</td>\n",
       "      <td>0.608631</td>\n",
       "      <td>0.75</td>\n",
       "      <td>63400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>No</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.257585</td>\n",
       "      <td>24.967657</td>\n",
       "      <td>1.005839</td>\n",
       "      <td>0.626672</td>\n",
       "      <td>1.218653</td>\n",
       "      <td>1.386292</td>\n",
       "      <td>0.608992</td>\n",
       "      <td>1.00</td>\n",
       "      <td>84534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Yes</td>\n",
       "      <td>1.437455e-07</td>\n",
       "      <td>1.257582</td>\n",
       "      <td>25.245032</td>\n",
       "      <td>0.996043</td>\n",
       "      <td>0.627665</td>\n",
       "      <td>1.214831</td>\n",
       "      <td>1.386286</td>\n",
       "      <td>0.609865</td>\n",
       "      <td>1.00</td>\n",
       "      <td>84534</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Promotion      purchase        V1         V2        V3        V4        V5  \\\n",
       "0        No  1.437455e-07  1.256658  26.010979  1.008264  0.619052  1.245876   \n",
       "1       Yes  1.437455e-07  1.263755  25.405717  1.012895  0.625228  1.210419   \n",
       "0        No  1.437455e-07  1.259141  24.944699  1.006074  0.627840  1.222414   \n",
       "1       Yes  1.437455e-07  1.255337  25.274806  0.992484  0.628158  1.217535   \n",
       "0        No  1.437455e-07  1.253815  25.146281  1.000132  0.629407  1.221187   \n",
       "1       Yes  1.437455e-07  1.257019  25.110459  0.990591  0.627671  1.216101   \n",
       "0        No  1.437455e-07  1.254171  25.105386  1.004506  0.626354  1.220354   \n",
       "1       Yes  1.437455e-07  1.258327  25.236680  0.994788  0.627159  1.215619   \n",
       "0        No  1.437455e-07  1.257585  24.967657  1.005839  0.626672  1.218653   \n",
       "1       Yes  1.437455e-07  1.257582  25.245032  0.996043  0.627665  1.214831   \n",
       "\n",
       "         V6        V7  prop_sampling  sample_size  \n",
       "0  1.385353  0.605214           0.05         4227  \n",
       "1  1.386085  0.617773           0.05         4227  \n",
       "0  1.386248  0.609784           0.25        21134  \n",
       "1  1.386273  0.609498           0.25        21134  \n",
       "0  1.386239  0.610107           0.50        42267  \n",
       "1  1.386261  0.608508           0.50        42267  \n",
       "0  1.386285  0.609656           0.75        63400  \n",
       "1  1.386270  0.608631           0.75        63400  \n",
       "0  1.386292  0.608992           1.00        84534  \n",
       "1  1.386286  0.609865           1.00        84534  "
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outcome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Move towards an even split\n",
    "\n",
    "We can look at the dataset created from the **increasing sample size**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Reduce variance in the metric definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. CUPED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare the reduced variance of each technique with the original variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
